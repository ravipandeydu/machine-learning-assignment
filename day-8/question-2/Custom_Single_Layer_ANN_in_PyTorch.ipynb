{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# main_script.py\n",
        "\n",
        "# --- 1. Import necessary libraries ---\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# --- 2. Data Generation and Loading ---\n",
        "# This section generates a synthetic dataset for binary classification\n",
        "# and saves it to a CSV file if it doesn't already exist.\n",
        "\n",
        "def generate_or_load_data(filename='binary_data.csv'):\n",
        "    \"\"\"\n",
        "    Generates a synthetic binary classification dataset if the file doesn't exist.\n",
        "    Loads the data from the CSV file into a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"'{filename}' not found. Generating a new dataset.\")\n",
        "        # Generate a dataset with 100 samples, 2 input features, and 2 classes.\n",
        "        X, y = make_classification(\n",
        "            n_samples=100,\n",
        "            n_features=2,\n",
        "            n_informative=2,\n",
        "            n_redundant=0,\n",
        "            n_classes=2,\n",
        "            random_state=1\n",
        "        )\n",
        "        # Create a DataFrame and save it to CSV.\n",
        "        df = pd.DataFrame(X, columns=['feature_1', 'feature_2'])\n",
        "        df['label'] = y\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Dataset saved to '{filename}'.\")\n",
        "    else:\n",
        "        print(f\"Loading existing dataset from '{filename}'.\")\n",
        "\n",
        "    # Load the data from the CSV file.\n",
        "    return pd.read_csv(filename)\n",
        "\n",
        "# --- 3. Model, Loss, and Training Functions (Manual Implementation) ---\n",
        "# We define our functions from scratch to avoid using torch.nn.\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "def binary_cross_entropy_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Binary Cross-Entropy loss function.\n",
        "    We add a small epsilon value to prevent log(0) which results in NaN.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-7\n",
        "    # Clamp predictions to avoid log(0) or log(1) issues\n",
        "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
        "    # BCE formula\n",
        "    loss = -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
        "    return loss\n",
        "\n",
        "# --- 4. Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Data Preparation ---\n",
        "    # Load data and get features (X) and labels (y)\n",
        "    data_df = generate_or_load_data()\n",
        "    X = data_df[['feature_1', 'feature_2']].values\n",
        "    y = data_df['label'].values\n",
        "\n",
        "    # Split data into training (80%) and testing (20%) sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # --- Convert to PyTorch Tensors and Move to Device ---\n",
        "    # Set device to GPU if available, otherwise CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"\\nUsing device: '{device}'\")\n",
        "\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "    # --- Hyperparameters ---\n",
        "    n_features = X_train_tensor.shape[1]\n",
        "    learning_rate = 0.1\n",
        "    epochs = 100\n",
        "\n",
        "    # --- Model Initialization (Manual) ---\n",
        "    # Initialize weights and bias.\n",
        "    # We set requires_grad=True to enable automatic gradient computation.\n",
        "    weights = torch.randn(n_features, 1, device=device, requires_grad=True, dtype=torch.float32)\n",
        "    bias = torch.zeros(1, device=device, requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(epochs):\n",
        "        # --- Forward Pass ---\n",
        "        # 1. Calculate the linear combination (Y = w^T * X + b)\n",
        "        linear_output = X_train_tensor @ weights + bias\n",
        "        # 2. Apply the sigmoid activation function\n",
        "        y_pred = sigmoid(linear_output)\n",
        "\n",
        "        # --- Calculate Loss ---\n",
        "        loss = binary_cross_entropy_loss(y_train_tensor, y_pred)\n",
        "\n",
        "        # --- Backward Pass ---\n",
        "        # PyTorch automatically calculates the gradients of the loss\n",
        "        # with respect to the tensors that have requires_grad=True (weights and bias).\n",
        "        loss.backward()\n",
        "\n",
        "        # --- Manual Weight Update (Gradient Descent) ---\n",
        "        # We wrap this in torch.no_grad() because we don't want to track\n",
        "        # this operation in the computation graph.\n",
        "        with torch.no_grad():\n",
        "            weights -= learning_rate * weights.grad\n",
        "            bias -= learning_rate * bias.grad\n",
        "\n",
        "            # --- Zero the Gradients ---\n",
        "            # It's crucial to zero the gradients after each update,\n",
        "            # otherwise they will accumulate on subsequent backward passes.\n",
        "            weights.grad.zero_()\n",
        "            bias.grad.zero_()\n",
        "\n",
        "        # Print loss every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(\"--- Training Finished ---\\n\")\n",
        "\n",
        "    # --- Evaluation on Test Set ---\n",
        "    with torch.no_grad():\n",
        "        # Make predictions on the test data\n",
        "        test_linear_output = X_test_tensor @ weights + bias\n",
        "        test_pred_probs = sigmoid(test_linear_output)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) by thresholding at 0.5\n",
        "        test_pred_labels = (test_pred_probs >= 0.5).float()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct_predictions = (test_pred_labels == y_test_tensor).sum().item()\n",
        "        total_samples = len(y_test_tensor)\n",
        "        accuracy = (correct_predictions / total_samples) * 100\n",
        "        print(f\"Accuracy on test set: {accuracy:.2f}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'binary_data.csv' not found. Generating a new dataset.\n",
            "Dataset saved to 'binary_data.csv'.\n",
            "\n",
            "Using device: 'cpu'\n",
            "\n",
            "--- Starting Training ---\n",
            "Epoch 10/100, Loss: 0.8364\n",
            "Epoch 20/100, Loss: 0.5942\n",
            "Epoch 30/100, Loss: 0.4539\n",
            "Epoch 40/100, Loss: 0.3695\n",
            "Epoch 50/100, Loss: 0.3151\n",
            "Epoch 60/100, Loss: 0.2775\n",
            "Epoch 70/100, Loss: 0.2501\n",
            "Epoch 80/100, Loss: 0.2291\n",
            "Epoch 90/100, Loss: 0.2126\n",
            "Epoch 100/100, Loss: 0.1992\n",
            "--- Training Finished ---\n",
            "\n",
            "Accuracy on test set: 100.00%\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxdPXF-1NWRA",
        "outputId": "3d912776-dc39-4230-8275-926a9368e7dd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}